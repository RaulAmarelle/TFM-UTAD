---
title: "Trabajo Final Master"
author: "Raul Amarelle"
date: "21 de agosto de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Cargamos librerias necesarias para todo el archivo

```{r}
library(readxl)
library(cluster)
library(ggplot2)
library(mclust)
library(fpc)
library(dplyr)
library(stats)
library(lubridate)
library(CausalImpact)
library(data.table)
library(bsts)
library(car)
library(zoo)
library(rpart) # decision tree method
library(rpart.plot) # tree plot
library(party) # decision tree method
library(forecast) # forecasting methods
library(tseries)
library(MLmetrics)
library(grid) # visualizations
library(animation) # gif
library(reshape)

```


En primer lugar, leemos los datos de nuestro data set

```{r}

data <- read_excel("../dat/Datos.xls")
df.data <- as.data.frame(data)
View(df.data)
```


############################
###  ANALISIS VARIABLES  ###
############################

Este dataset es una recopilacion de datos diarios de temperatura y potencia consumida relativos a un centro comercial.

Breve explicacion de las variables:
1. Fecha: fecha de la medicion, hay una medicion diaria
2. Estimado: no sirve de nada
3. Kwh: potencia consumida
4. LB: Linea base, es decir, la prediccion de potencia consumida
5 a 10. CCD o CCHD: Cooling-Degree Day o Cooling Heating Degree Day son temperaturas recogidas en 6 columnas. Es la diferencia entre el promedio diario de temperatura y una determinada temperatura base de referencia (suele ser la exterior). El centro comercial tiene dos modos de climatizacion: calentar o enfriar, por eso son siempre 3 ceros vs 3 numeros. 18, 19 y 20 es la temperatura base en ambos casos. Por eso el valor es siempre el mismo sumando o restando 1 de una columna a la siguiente.
11. Afluencia: Numero de personas visitantes.


Quiere decir que de las 3+3 columnas de temperatura, 2+2 son redundantes. Con quedarnos una columna de cada modo de climatizacion, es suficiente. Escogemos CCDD20 y CHDD18, porque son las columnas mas ventajosas.


##############################
###  TRATAMIENTO DE DATOS  ###
##############################

1. Eliminamos las variables que no aportan informacion

```{r}
# segun lo comentado anteriormente

df.data <- df.data[,-c(2, 5, 6, 9, 10)]
View(df.data)
```


2. Hacemos un preanalisis de variables

```{r}
dim(df.data)
str(df.data)
summary(df.data)
```


3. Creacion de nuevas variables

```{r}
# Vamos a crear nuevas columnas para dia, mes y anio, nos pueden ser de utilidad para segmentar las predicciones segun periodo. 

df.data$DAY <- 'NULO'
df.data$MONTH <- 'NULO'
df.data$YEAR <- 'NULO'

for (i in c(1:nrow(df.data))){
  my_date <- dmy(df.data$FECHA[i])
  df.data$DAY[i] <- day(my_date)
  df.data$MONTH[i] <- month(my_date)
  df.data$YEAR[i] <- year(my_date)
}

str(df.data)
View(df.data)
```

```{r}
# Las nuevas columnas creadas son caracteres. La variable Fecha tambien. Las cambiamos de clase, a numero y Date respectivamente.

df.data$FECHA <- as.Date(df.data$FECHA, format="%d/%m/%Y")
df.data$YEAR <- as.numeric(as.character(df.data$YEAR))
df.data$MONTH <- as.numeric(as.character(df.data$MONTH))
df.data$DAY <- as.numeric(as.character(df.data$DAY))

str(df.data)
View(df.data)
```

```{r}
## Puede ser interesante crear una columna categorica Entre semana (ES) y Fin de semana (FS), porque los fines de semana hay previsiblemente mas asistencia

# Primero creamos la variable WeekDay como ('L', 'M', 'X', 'J', 'V', 'S', 'D') segun corresponda
df.data$WEEKDAY <- 'NULO'

seq1 <- c('M', 'X', 'J', 'V', 'S', 'D', 'L')
df.data$WEEKDAY[1:728] <- seq1
df.data$WEEKDAY[729] <- 'M'

# Creamos la variable categorica entre semana (ES) y fin de semana (FS)
df.data$FINDE <- ifelse(df.data$WEEKDAY %in% c('S', 'D'), 'FS', 'ES')

View(df.data)
```

```{r}
# Creamos la variable categorica Climatizacion, que tomara 3 valores:
#   - Heating
#   - Cooling
#   - Neutro

df.data$CLIMATIZACION <- ifelse(df.data$CCDD20 > 0, 'Cooling',
                                         ifelse(df.data$CHDD18 > 0, 'Heating', 'Neutro'))

View(df.data)
```

```{r}
# Creamos una nueva columna, que la llamaremos ERROR, que sera el % de error de la potencia consumida vs prevision de potencia
df.data$ERROR <- (df.data$KWH-df.data$LB)/df.data$KWH * 100

View(df.data)
```


4. Tratamiento de Missing Values

```{r}
# Vamos a calcular los N/A del dataset
nas <- nrow(data[data$CHDD18!=0 & data$CCDD20!=0,])
nas

# Solo hay 4 muestras N/A, aprox el 0.5% de los datos
nas / nrow(df.data) *100

# Considero que puedo eliminar esas filas porque, aparte de ser un numero muy bajo y poco representativo, no se puede saber si ese dia el sistema funciono bajo regimen de calefaccion o refrigeracion, por lo que no podre extraer conclusiones
df.data <- df.data[!is.na(df.data$CCDD20),]

View(df.data)
```


5. Visualizacion de datos 

```{r}
# Distribucion con funciones de densidad
ggplot() +
  geom_density(data = df.data, aes(x = scale(KWH), colour = 'kWh')) +
  geom_density(data = df.data, aes(x = scale(LB), colour = 'LB')) +
  geom_density(data = df.data, aes(x = scale(CCDD20), colour = 'CCDD20')) +
  geom_density(data = df.data, aes(x = scale(CHDD18), colour = 'CHDD18')) + 
  geom_density(data = df.data, aes(x = scale(AFLUENCIA), colour = 'AFLUENCIA')) + 
  scale_colour_manual('', breaks = c('KWH', 'LB', 'CCDD20', 'CHDD18', 'AFLUENCIA'), values = c('red', 'blue', 'black', 'yellow', 'green'))
```


```{r}
# Ploteo de las variables 

ggplot(data = df.data, aes(x=FECHA, y= KWH)) + geom_line(colour = 'red') + geom_line(aes(x=FECHA, y=LB), colour = "blue") + 
  geom_line(aes(x=FECHA, y=CCDD20), colour = 'black') + 
  geom_line(aes(x=FECHA, y=CHDD18), colour = 'yellow') +
  geom_line(aes(x=FECHA, y=AFLUENCIA), colour = 'green') 

```


```{r}
# # Comparativa: Consumo KWH (real, negro) vs LB(prediccion, rojo). La linea azul es la tendencia de la LB

g <- ggplot(df.data, aes(x=FECHA, y=KWH)) +  geom_line() + geom_line(aes(x=FECHA, y=LB), linetype = 5, color = "red") + geom_smooth(method="lm")
plot(g)

```

Se aprecia que hay una progresiva tendencia de disminucion del consumo a lo largo de estos dos anios, las medidas de ahorro energetico estan funcionando


```{r}
# Diagrama de barras de la variable Climatizacion
ggplot(data = df.data, aes(x = CLIMATIZACION)) + geom_bar(aes(fill = CLIMATIZACION))
```

```{r}
# Analisis Afluencia, agrupada por dia de la semana
# suma
aggregate(formula = AFLUENCIA ~ WEEKDAY, data = df.data, FUN = sum)
# media
aggregate(formula = AFLUENCIA ~ WEEKDAY, data = df.data, FUN = mean)

# AFLUENCIA agrupada por mes
# media
aggregate(formula = AFLUENCIA ~ MONTH, data = df.data, FUN = mean)
```

```{r}
# POTENCIA (kWh)
# agrupada por fin de semana/dia de la semana
aggregate(formula = KWH ~ FINDE, data = df.data, FUN = mean)
# agrupada por meses
aggregate(formula = KWH ~ MONTH, data = df.data, FUN = mean)
```


6. Analisis correlaciones

```{r}
# Vamos a ver la matriz de correlaciones lineales entre KWH (2), CCDD20(4), CHDD18(5) y AFLUENCIA (6)

pairs(~KWH+CCDD20+CHDD18+AFLUENCIA,data=df.data, main="Scatterplot Matrix")

```

```{r}
# Calculamos las correlaciones entre las variables anteriores

correlation <- cor(df.data[,c(2,4,5,6)])
correlation
```


#############################################
###  CASOS DE ANALISIS SERIES TEMPORALES  ###
#############################################

# CASO 1: Impacto causal utilizando modelos bayesianos de series de tiempo

```{r}
# Analizamos la evolucion de la variable Error

ST_Error=ts(df.data$ERROR, start= c(2016,3,1), end=c(2018,2,27), frequency = 365)
plot(ST_Error, xlab= "Periodo", ylab="% Error")

```

Como primera aproximacion, podemos observar que el modelo de prediccion ha mejorado a partir de la segunda mitad de anio, ya que el % Error es mucho menor, tiende a concentrarse en la horquilla de -10% a +10%.

Vamos a suponer que a partir de la medicion 458, coincidiendo con el 01/06/2017, se han introducido mejoras en el algoritmo de prediccion y vamos a comparar la mejora

```{r}
y <- window(df.data$KWH, start=1, end=725)
```

```{r}
post.period <- c(457:725)
post.period.response <- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] <- NA
```

```{r}
ss <- AddLocalLevel(list(), y)
bsts.model <- bsts(y, ss, niter = 1000)
```


```{r}
# ERROR
# impact <- CausalImpact(bsts.model = bsts.model, post.period.response = post.period.response)
# plot(impact)

```

```{r}
# Depende del anterior
# summary(impact)
# summary(impact, "report")
```


# CASO 2: Metodo Holt-Winters

Vamos a hacer predicciones segun el Metodo Holt-Winters, que se caracteriza por hacer pronosticos utilizando un suavizado exponencial simple

```{r}
# Representamos los datos de consumo KWH
PotKWH <- ts(df.data$KWH, start= c(2016,03,01), end= c(2018,02,27), frequency = 365)
plot(PotKWH)

```

```{r}
# Se pasa la serie temporal a HoltWinter y se trazan los datos ajustados.
hw <- HoltWinters(PotKWH, beta = FALSE , gamma = FALSE)
hw   # Para ver los parametros
plot(hw)
```

Como alpha es 0.9999 practicamente no hay amortiguación de la serie

```{r}
# Las predicciones son almacenadas bajo la variable fitted
head(hw$fitted)

# Suma de errores cuadráticos
head(hw$SSE)
```


Calculamos la previsión para los proximos 3 meses con un intervalo de confianza de 0,95 y representamos el pronóstico junto con los valores reales y ajustados.

```{r}
forecast.hw <- predict(hw, n.ahead = 90, prediction.interval = T, level = 0.95)
plot(hw, forecast.hw)
```


Para hacer un analisis más certero de las predicciones usando el método Holt-Winters, se puede usar el paquete ggplot2. 

```{r}
# Usamos esta funcion que extrae algunos datos de los objetos HoltWinter y predict.HoltWinter y los alimenta a ggplot2
library(reshape)
 
 
HWggplot2<-function(ts_object,  n.ahead=12,  CI=.95,  error.ribbon='red', line.size=1){
     
    hw_object<-HoltWinters(ts_object, beta = FALSE , gamma = FALSE)
     
    forecast<-predict(hw_object,  n.ahead=n.ahead,  prediction.interval=T,  level=CI)
     
     
    for_values<-data.frame(time=round(time(forecast),  3),  value_forecast=as.data.frame(forecast)$fit,  dev=as.data.frame(forecast)$upr-as.data.frame(forecast)$fit)
     
    fitted_values<-data.frame(time=round(time(hw_object$fitted),  3),  value_fitted=as.data.frame(hw_object$fitted)$xhat)
     
    actual_values<-data.frame(time=round(time(hw_object$x),  3),  Actual=c(hw_object$x))
     
     
    graphset<-merge(actual_values,  fitted_values,  by='time',  all=TRUE)
    graphset<-merge(graphset,  for_values,  all=TRUE,  by='time')
    graphset[is.na(graphset$dev),  ]$dev<-0
     
    graphset$Fitted<-c(rep(NA,  NROW(graphset)-(NROW(for_values) + NROW(fitted_values))),  fitted_values$value_fitted,  for_values$value_forecast)
     
     
    graphset.melt<-melt(graphset[, c('time', 'Actual', 'Fitted')], id='time')
     
  p<-ggplot(graphset.melt,  aes(x=time,  y=value)) + geom_ribbon(data=graphset, aes(x=time, y=Fitted, ymin=Fitted-dev,  ymax=Fitted + dev),  alpha=.2,  fill=error.ribbon) + geom_line(aes(colour=variable), size=line.size) + geom_vline(xintercept=max(actual_values$time),  lty=2) + xlab('Time') + ylab('Value') + labs(legend.position='bottom') + scale_colour_hue('')
    return(p)
 
}
```


Recuperamos la serie inicial, pero le aplicamos la funcion HWggplot2
```{r}
graph <- HWggplot2(PotKWH, n.ahead = 90)
graph

```

```{r}
# Ponemos leyendas al grafico
graph <- graph + labs(title = "Predicciones Método Holt-Winters en ggplot2")
graph <- graph + ylab("Potencia KWH")
graph <- graph + xlab("Tiempo")

graph

```


# CASO 3: METODO ARIMA

Paso 1: Examinar datos

```{r}
ggplot(df.data, aes(FECHA, KWH)) + geom_line() + scale_x_date('MES')  + ylab("Consumo KWH") + xlab("")
```

```{r}
# Consideramos que no hay una relacion directa entre el consumo de un dia y del siguiente, la temperatura puede oscilar, la afluencia puede cambiar mucho, etc. Por si el dataset tuviera algunos valores volatiles, voy a eliminar valores atipicos de series temporales

KWH_ts = ts(df.data[, c('KWH')])

df.data$clean_KWH= tsclean(KWH_ts)

ggplot() + geom_line(data = df.data, aes(x = FECHA, y = clean_KWH)) + ylab('Consumo KWH Limpio')

```

Los datos son bastante similares.
Para suavizar las fluctuaciones de la curva utilizamos el promedio movil.
Cuanto mas ancha es la ventana de la media movil, mas suave se vuelve la serie original. En nuestro caso, podemos tomar la media movil semanal o mensual, suavizando la serie en algo mas estable y, por lo tanto, mas predecible.

```{r}
# Promedio movil o en ingles Mobile Average (ma)

df.data$KWH_ma = ma(df.data$clean_KWH, order=7) 
df.data$KWH_ma30 = ma(df.data$clean_KWH, order=30)


ggplot() + geom_line(data = df.data, aes(x = FECHA, y = clean_KWH, colour = "Consumo KWH Limpio")) +
  geom_line(data = df.data, aes(x = FECHA, y = KWH_ma, colour = "Promedio Movil Semanal"))  +
  geom_line(data = df.data, aes(x = FECHA, y = KWH_ma30, colour = "Promedio Movil Mensual"))  +
  ylab('Consumo')

```


# Paso 2: Descomposicion de la serie

Los componentes basicos de un analisis de series de tiempo son la estacionalidad, la tendencia y el ciclo. El proceso de extraccion de estos componentes se conoce como descomposicion

```{r}
# Calculamos el componente estacional de los datos usando stl()

Consumo_ma = ts(na.omit(df.data$KWH_ma), frequency=30)
decomp = stl(Consumo_ma, s.window="periodic")
deseasonal_KWH <- seasadj(decomp)
plot(decomp)

```

# Paso 3: Estacionariedad

La instalacion de un modelo ARIMA requiere que la serie sea estacionaria. La prueba aumentada Dickey-Fuller (ADF) es una prueba estadistica formal para la estacionalidad, donde la hipotesis nula supone que la serie no es estacionaria

```{r}
adf.test(Consumo_ma, alternative = "stationary")
```

Con estos valores no queda claro si es estacionaria o no


# Paso 4: Diagramas de autocorrelacion

Los diagramas de autocorrelacion son una herramienta visual util para determinar si una serie es estacionaria. Estas parcelas tambien pueden ayudar a elegir los parametros de orden para el modelo ARIMA.

```{r}
# Identificacion de modelos ARIMA (p,d,q)
# ACF: auto correlation function, nos ayuda con el modelo q
Acf(Consumo_ma, main='')

# PACF: Partial autocorrelation function, nos ayuda con el modelo p
Pacf(Consumo_ma, main='')
```

En la segunda grafica vemos un patron oscilante alrededor de 0 sin una tendencia fuerte visible. Esto sugiere que la diferenciacion de los terminos del orden 1 (d=1) es suficiente y debe incluirse en el modelo.

```{r}
Consumo_d1 = diff(deseasonal_KWH, differences = 1)
plot(Consumo_d1)
adf.test(Consumo_d1, alternative = "stationary")

```

Si observamos esta serie, esta serie diferenciada tiende a estacionaria. Los picos en momentos particulares pueden ayudar a informar la eleccion de p o q  para nuestro modelo

```{r}
Acf(Consumo_d1, main='ACF para Series Diferenciadas')
Pacf(Consumo_d1, main='PACF para Series Diferenciadas')
```


# Paso 5: Ajustar el modelo ARIMA

La funcion arima nos permite especificar explicitamente los parametros del modelo o generar automaticamente un conjunto optimo de (p, d, q) usando auto.arima (). Esta funcion busca a traves de combinaciones de parametros y selecciona el conjunto que optimiza los criterios de ajuste del modelo.

```{r}
auto.arima(deseasonal_KWH, seasonal=FALSE)
```

Coeficiente ar1 p = 0.7436 nos dice que el siguiente valor de la serie se toma como un valor anterior atenuado por un factor de 0,7436 y depende del retardo de error anterior.


# Paso 6: Evaluar e iterar

Vamos a examinar los graficos ACF y PACF para los residuos del modelo

```{r}
fit1<-auto.arima(deseasonal_KWH, seasonal=FALSE)
tsdisplay(residuals(fit1), lag.max=45, main='(5,1,4) Residuos del Modelo')
```

Hay un claro patron presente en ACF, FAP y Residuos del Modelo que se repite en el retardo 7. Esto indica que nuestro modelo puede mejorarse con un nuevo parametro q = 7.

```{r}
fit2 = arima(deseasonal_KWH, order=c(5,1,7))

fit2

tsdisplay(residuals(fit2), lag.max=15, main='Residuales Modelo Estacional')

```


# Paso 7: Pronosticos

Podemos especificar el horizonte de pronostico h (periodos por delante) para que se realicen las predicciones usando el modelo ajustado.

```{r}
fcast <- forecast(fit2, h=60)
plot(fcast)
```

Otra opcion es superponer los datos reales con las previsiones en el ultimo bloque de unidades temporales

```{r}
# Calculo prediccion para los proximos 5 meses, a partir 01/10/17
soporte <- window(ts(deseasonal_KWH), start=578)

fit_sin_soporte = arima(ts(deseasonal_KWH[-c(578:725)]), order=c(5,1,7))

prediccion_sin_soporte <- forecast(fit_sin_soporte,h=150)
plot(prediccion_sin_soporte, main=" ")
lines(ts(deseasonal_KWH))
```

Las predicciones no son muy buenas. Como mejorar el modelo?
Por ejemplo vamos a utilizar la funcion auto.arima con estacionaliad

```{r}
fit_con_estacionalidad = auto.arima(deseasonal_KWH, seasonal=TRUE)
fit_con_estacionalidad

```

Los parametros optimizados con auto.arima son (3,1,3)

```{r}
prediccion_con_estacionalidad <- forecast(fit_con_estacionalidad, h=30)
plot(prediccion_con_estacionalidad)
```

Calculamos de nuevo los residuales con los parametros optimizados con auto.arima

```{r}
fit3 = arima(deseasonal_KWH, order=c(3,1,3))
fit3

tsdisplay(residuals(fit3), lag.max=15, main='Residuales Modelo Estacional')

```


# CASO 4: Arboles de clasificacion y regresion, RPART (CART) Tree

1. TREE 1

```{r}
# Entrenamos el modelo quitando la fecha, potencia estimada (LB), DAY, YEAR

tree1 <- rpart(KWH ~ . , data = df.data[,-c(1,3,7,9,13)] )
```

```{r}
# Funcion para normalizar datos. El minimo a 0, el maximo a 1

normalize <- function(x){
  x <- (x - min(x))/(max(x)-min(x))
  return(x)
}
```

```{r}
# Observamos la importancia de cada variable en una primera aproximacion.

tree1$variable.importance
normalize(tree1$variable.importance)
```


```{r}
# Representamos el arbol

rpart.plot(tree1, digits = 2,
           box.palette = viridis::viridis(10, option = "D", begin = 0.85, end = 0), 
           shadow.col = "grey65", col = "grey99")
```


Hacemos la prediccion de la serie

```{r}
# Primero creamos un data frame con los valores reales y las predicciones:

predictions.tree1 = data.frame(matrix(nrow = 725, ncol = 3))
colnames(predictions.tree1) = c('index', 'Real', 'Estimated.RPART')
predictions.tree1$index = seq(1,725,1)
predictions.tree1$Real = df.data$KWH
predictions.tree1$Estimated.RPART = predict(tree1) 
```


```{r}
# Ploteamos la prediccion vs el valor real a ver como queda:

ggplot() +
  geom_line(data = predictions.tree1, aes(x = index, y = Real, color = 'Real')) + 
  geom_line(data = predictions.tree1, aes(x = index, y = Estimated.RPART, color = 'Estimated.RPART')) +
  scale_colour_manual('', breaks = c('Real', 'Estimated.RPART'), values = c('red', 'blue'))
              
```

Vemos que RPART superpone bien la tendencia de la serie real, pero en los picos no es capaz de predecir bien. Como metrica para evaluar la prediccion
de la serie utilizamos la metrica MAPE (mean absolute percentage error). Queda bien explicado aquí: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error

```{r}
error.tree1 <- MAPE(predictions.tree1$Real, predictions.tree1$Estimated.RPART)
error.tree1
```

El error es razonable dentro de lo que cabe, pero para mejorar predicciones hay que tunear un poco nuestro tree1 con rpart.control.


2. TREE 2

Vamos a especificar los siguientes parámetros:
- minsplit
- maxdepth
- cp


```{r}
tree2 <- rpart(KWH ~ . , data = df.data[,-c(1,3,7,9,13)],
               control = rpart.control(minsplit = 2,
                                       max_depth = 20,
                                       cp = 0.00001))
```


Representamos el arbol, es bueno ver la comparacion a nivel de complejidad con el tree1, del que esperamos que tenga peores predicciones.

```{r}
plot(tree1)
plot(tree2, compress = TRUE)
```

Chequeamos el numero de splits y los comparamos con tree1

```{r}
tree2$cptable[dim(tree2$cptable)[1], 'nsplit'] # 468
tree1$cptable[dim(tree1$cptable)[1], 'nsplit'] # 5
```

Creammos nuevo data frame con las predicciones de tree2 y los valores reales

```{r}
predictions.tree2 = data.frame(matrix(nrow = 725, ncol = 3))
colnames(predictions.tree2) = c('index', 'Real', 'Estimated.RPART')
predictions.tree2$index = seq(1,725,1)
predictions.tree2$Real = df.data$KWH
predictions.tree2$Estimated.RPART = predict(tree2) 
```


Ploteamos la prediccion vs el valor real

```{r}
ggplot() +
  geom_line(data = predictions.tree2, aes(x = index, y = Real, color = 'Real')) + 
  geom_line(data = predictions.tree2, aes(x = index, y = Estimated.RPART, color = 'Estimated.RPART')) +
  scale_colour_manual('', breaks = c('Real', 'Estimated.RPART'), values = c('red', 'blue'))
              
```

Metrica MAPE
```{r}
error.tree2 <- MAPE(predictions.tree2$Real, predictions.tree2$Estimated.RPART)
error.tree2
```

Se ha mejorado muchisimo la prediccion, pasando de un MAPE de 6.8 a un MAPE de 0.39

Vamos a poner una combinacion de los hiperparametros de control para ver cual es la configuracion que mejor predice

```{r}
prueba.cp <- c(0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001)
mape.pruebas.cp <- c() # 
for (i in prueba.cp){
  tree.aux <- rpart(KWH ~ . , data = df.data[,-c(1,3,7,9,13)],
                    control = rpart.control(minsplit = 2,
                                       max_depth = 20,
                                       cp = i))
  mape.aux <- MAPE(df.data$KWH, predict(tree.aux))
  mape.pruebas.cp <- append(mape.pruebas.cp, mape.aux)
}
```

A medida que reducimos el cp, el MAPE baja, aunque esto es un indicio de que el modelo esta bajo overfitting.

Para ver que tal estamos haciendo esto tenemos que segmentar los datos en un training y un test, no vale predecir sobre valores con los que hemos entrenado. Ademas los arboles de decision son muy propensos al overfiting. Si analizamos, en tree2 teniamos 468 splits, mas de la mitad de las observaciones que tenemos. Practicamente estamos haciendo 2 splits para cada 3 observaciones, con tantos  splits no seria operativo.

Vamos a crear 2 set de training y 2 set de test:
- data.train.1: 1/03/2016 -> 31/03/2017
- data.test.1: 1/04/2017 -> 30/04/2017
- data.train.2: 1/01/2017 -> 31/01/2018
- data.test.2: 1/02/2018 -> 27/02/2018


```{r}
# Y de paso le quitamos las columnas que hemos dicho que no queriamos para entrenar los arboles

data.train.1 <- df.data[1:395,-c(1,3,7,9,13)]
data.test.1 <- df.data[396:425,-c(1,3,7,9,13)]
data.train.2 <- df.data[306:700,-c(1,3,7,9,13)]
data.test.2 <- df.data[701:725,-c(1,3,7,9,13)]

```


Probamos con Rpart con data.train.1 y data.test.1
```{r}
tree.data.1 <- rpart(KWH ~ . , data = data.train.1,
               control = rpart.control(minsplit = 2,
                                       max_depth = 20,
                                       cp = 0.00001))

predictions.data.1 <- data.frame(matrix(nrow = 30, ncol = 3))
colnames(predictions.data.1) = c('index', 'Real', 'Estimated.RPART')
predictions.data.1$index <- seq(1,30,1)
predictions.data.1$Real <- df.data[396:425, 2]
predictions.data.1$Estimated.RPART <- predict(tree.data.1, data.test.1)

```

Ploteamos a ver que tal sale
```{r}
ggplot() +
  geom_line(data = predictions.data.1, aes(x = index, y = Real, color = 'Real')) + 
  geom_line(data = predictions.data.1, aes(x = index, y = Estimated.RPART, color = 'Estimated.RPART')) +
  scale_colour_manual('', breaks = c('Real', 'Estimated.RPART'), values = c('red', 'blue'))
```

Metrica MAPE
```{r}
MAPE(predictions.data.1$Real, predictions.data.1$Estimated.RPART)
```

Probamos a tunear hiperparametros, por si podemos mejorar

```{r}
minsplit.data.1 <- c(2,3,4,5,6)
max_depth.data.1 <- c(5,7,9,11,13,15,17,21,23,25,27,30)
cp.data.1 <- c(0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001)

mape.1 <- 10000
hp.1.min_spl <- 0
hp.1.max_dep <- 0
hp.1.cp <- 0

for (i in minsplit.data.1){
  for (j in max_depth.data.1){
    for (k in cp.data.1){
      tree.aux <- rpart(KWH ~ . , data = data.train.1,
                    control = rpart.control(minsplit = i,
                                       max_depth = j,
                                       cp = k))
      mape.aux <- MAPE(data.test.1$KWH, predict(tree.aux, data.test.1))
  
      if (mape.aux < mape.1){
        hp.1.min_spl <- i
        hp.1.max_dep <- j
        hp.1.cp <- k
      }
    }
  }
}
```


Nos sale el mejor Mape con min_split, max_depth al maximo y cp al minimo. Vamos a ver si Mape es monotona decreciente con al aumentar los dos primeros y disminuir el ultimo o si hay un minimo local.

```{r}
minsplit.data.1 <- c(8,10,12,14,16,18,20)
max_depth.data.1 <- c(30,40,50,60,70,80,90,100)
cp.data.1 <- c(1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13)

mape.1 <- 10000
hp.1.min_spl <- 0
hp.1.max_dep <- 0
hp.1.cp <- 0

for (i in minsplit.data.1){
  for (j in max_depth.data.1){
    for (k in cp.data.1){
      tree.aux <- rpart(KWH ~ . , data = data.train.1,
                    control = rpart.control(minsplit = i,
                                       max_depth = j,
                                       cp = k))
      mape.aux <- MAPE(data.test.1$KWH, predict(tree.aux, data.test.1))
  
      if (mape.aux < mape.1){
        hp.1.min_spl <- i
        hp.1.max_dep <- j
        hp.1.cp <- k
      }
    }
  }
}
```

Es interesante ver como varia el Mape a medida que disminuimos el cp y aumentamos los otros dos, igual es que no tiene influencia. Hay que ver tambien como la estructura del arbol va variando a medida que modificamos los hiperparametros. Parece que el unico parametro que tiene especial relevancia a partir de un punto es el cp.









